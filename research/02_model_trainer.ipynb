{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9faeb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ebf35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\shipr\\\\OneDrive\\\\Documents\\\\project\\\\Brain-tumor-Classification\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd2ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e8efc5",
   "metadata": {},
   "source": [
    "### Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bbd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for Model Training\"\"\"\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MLflowConfig:\n",
    "    \"\"\"Configuration for MLflow Tracking\"\"\"\n",
    "    experiment_name: str\n",
    "    run_name_prefix: str\n",
    "    tracking_uri: str\n",
    "    registered_model_name: str\n",
    "    tags: dict     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252013d3",
   "metadata": {},
   "source": [
    "### Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BT_Classification.constants import *\n",
    "from BT_Classification.utils.common import read_yaml, create_directories\n",
    "from BT_Classification.entity import (\n",
    "    DataIngestionConfig,PrepareBaseModelConfig,MLflowConfig,TrainingConfig\n",
    ")\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Configuration Manager to read and manage all configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Create artifacts root directory\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Get Data Ingestion configuration\n",
    "        \n",
    "        Returns:\n",
    "            DataIngestionConfig: Configuration object for data ingestion\n",
    "        \"\"\"\n",
    "        config = self.config.data_ingestion\n",
    "        \n",
    "        # Create root directory for data ingestion\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=Path(config.local_data_file),\n",
    "            unzip_dir=Path(config.unzip_dir),\n",
    "            train_data_dir=Path(config.train_data_dir),\n",
    "            test_data_dir=Path(config.test_data_dir)\n",
    "        )\n",
    "        \n",
    "        return data_ingestion_config\n",
    "    \n",
    "     \n",
    "    def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "        \"\"\"\n",
    "        Get Base Model Preparation configuration\n",
    "        \n",
    "        Returns:\n",
    "            PrepareBaseModelConfig: Configuration object for base model\n",
    "        \"\"\"\n",
    "        config = self.config.prepare_base_model\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        prepare_base_model_config = PrepareBaseModelConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            base_model_path=Path(config.base_model_path),\n",
    "            updated_base_model_path=Path(config.updated_base_model_path),\n",
    "            params_image_size=self.params.IMAGE_SIZE,\n",
    "            params_learning_rate=self.params.LEARNING_RATE,\n",
    "            params_include_top=self.params.INCLUDE_TOP,\n",
    "            params_weights=self.params.WEIGHTS,\n",
    "            params_classes=self.params.CLASSES\n",
    "        )\n",
    "        \n",
    "        return prepare_base_model_config\n",
    "    \n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        \"\"\"\n",
    "        Get Training configuration\n",
    "        \n",
    "        Returns:\n",
    "            TrainingConfig: Configuration object for model training\n",
    "        \"\"\"\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        training_data = self.config.data_ingestion.train_data_dir\n",
    "        \n",
    "        create_directories([Path(training.root_dir)])\n",
    "        \n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_is_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            params_learning_rate=params.LEARNING_RATE\n",
    "        )\n",
    "        \n",
    "        return training_config\n",
    "    \n",
    "    \n",
    "    def get_mlflow_config(self) -> MLflowConfig:\n",
    "        \"\"\"\n",
    "        Get MLflow configuration\n",
    "        \n",
    "        Returns:\n",
    "            MLflowConfig: Configuration object for MLflow tracking\n",
    "        \"\"\"\n",
    "        mlflow_config_data = self.config.mlflow\n",
    "        \n",
    "        mlflow_config = MLflowConfig(\n",
    "            experiment_name=mlflow_config_data.experiment_name,\n",
    "            run_name_prefix=mlflow_config_data.run_name_prefix,\n",
    "            tracking_uri=mlflow_config_data.tracking_uri if mlflow_config_data.tracking_uri else \"\",\n",
    "            registered_model_name=mlflow_config_data.registered_model_name,\n",
    "            tags=dict(mlflow_config_data.tags) if hasattr(mlflow_config_data, 'tags') else {}\n",
    "        )\n",
    "        \n",
    "        return mlflow_config\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76a60b",
   "metadata": {},
   "source": [
    "### 02_model_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3301e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "110dcee3",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE_NAME = \"Model Training Stage\"\n",
    "\n",
    "\n",
    "class ModelTrainingPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline for Model Training Stage\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def main(self):\n",
    "        \"\"\"Execute model training pipeline\"\"\"\n",
    "        config = ConfigurationManager()\n",
    "        training_config = config.get_training_config()\n",
    "        mlflow_config = config.get_mlflow_config()\n",
    "        training = Training(config=training_config, mlflow_config=mlflow_config)\n",
    "        training.initiate_model_training()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
    "        logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "        obj = ModelTrainingPipeline()\n",
    "        obj.main()\n",
    "        \n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\")\n",
    "        logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise e    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab4c42",
   "metadata": {},
   "source": [
    "### Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import dagshub\n",
    "from BT_Classification import logger\n",
    "from BT_Classification.entity import TrainingConfig, MLflowConfig\n",
    "\n",
    "\n",
    "class Training:\n",
    "    \"\"\"\n",
    "    Component for training the brain tumor classification model with MLflow tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, mlflow_config: MLflowConfig):\n",
    "        \"\"\"\n",
    "        Initialize Training component\n",
    "        \n",
    "        Args:\n",
    "            config (TrainingConfig): Configuration for model training\n",
    "            mlflow_config (MLflowConfig): Configuration for MLflow tracking\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.mlflow_config = mlflow_config\n",
    "    \n",
    "    \n",
    "    def get_base_model(self):\n",
    "        \"\"\"\n",
    "        Load the prepared base model with custom head\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading base model from: {self.config.updated_base_model_path}\")\n",
    "            \n",
    "            self.model = tf.keras.models.load_model(\n",
    "                str(self.config.updated_base_model_path)\n",
    "            )\n",
    "            \n",
    "            logger.info(\"✓ Model loaded successfully\")\n",
    "            logger.info(f\"✓ Model input shape: {self.model.input_shape}\")\n",
    "            logger.info(f\"✓ Model output shape: {self.model.output_shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    \n",
    "    def train_valid_generator(self):\n",
    "        \"\"\"\n",
    "        Create data generators for training and validation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Setting up data generators...\")\n",
    "            \n",
    "            # Data augmentation configuration for training\n",
    "            if self.config.params_is_augmentation:\n",
    "                logger.info(\"✓ Data augmentation enabled\")\n",
    "                train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    rotation_range=20,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.3,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    brightness_range=[0.8,1.2],\n",
    "                    fill_mode='nearest',\n",
    "                    validation_split=0.2  # 20% for validation\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\"✓ No data augmentation\")\n",
    "                train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    validation_split=0.2\n",
    "                )\n",
    "            \n",
    "            # Training generator\n",
    "            self.train_generator = train_datagen.flow_from_directory(\n",
    "                directory=str(self.config.training_data),\n",
    "                target_size=self.config.params_image_size[:-1],\n",
    "                batch_size=self.config.params_batch_size,\n",
    "                class_mode='categorical',\n",
    "                subset='training',\n",
    "                shuffle=True,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            # Validation generator (no augmentation)\n",
    "            self.validation_generator = train_datagen.flow_from_directory(\n",
    "                directory=str(self.config.training_data),\n",
    "                target_size=self.config.params_image_size[:-1],\n",
    "                batch_size=self.config.params_batch_size,\n",
    "                class_mode='categorical',\n",
    "                subset='validation',\n",
    "                shuffle=False,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            # Log dataset information\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(\"DATASET INFORMATION\")\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(f\"✓ Training samples: {self.train_generator.samples}\")\n",
    "            logger.info(f\"✓ Validation samples: {self.validation_generator.samples}\")\n",
    "            logger.info(f\"✓ Number of classes: {self.train_generator.num_classes}\")\n",
    "            logger.info(f\"✓ Class indices: {self.train_generator.class_indices}\")\n",
    "            logger.info(f\"✓ Batch size: {self.config.params_batch_size}\")\n",
    "            logger.info(f\"✓ Steps per epoch: {self.train_generator.samples // self.config.params_batch_size}\")\n",
    "            logger.info(f\"✓ Validation steps: {self.validation_generator.samples // self.config.params_batch_size}\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        \"\"\"\n",
    "        Save the trained model\n",
    "        \n",
    "        Args:\n",
    "            path (Path): Path to save the model\n",
    "            model (tf.keras.Model): Trained model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model.save(str(path))\n",
    "            logger.info(f\"✓ Model saved at: {path}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model with MLflow tracking\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(\"STARTING MODEL TRAINING WITH MLFLOW\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            # Calculate steps\n",
    "            steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size\n",
    "            validation_steps = self.validation_generator.samples // self.validation_generator.batch_size\n",
    "            \n",
    "            logger.info(f\"\\nTraining Configuration:\")\n",
    "            logger.info(f\"  → Epochs: {self.config.params_epochs}\")\n",
    "            logger.info(f\"  → Batch Size: {self.config.params_batch_size}\")\n",
    "            logger.info(f\"  → Steps per Epoch: {steps_per_epoch}\")\n",
    "            logger.info(f\"  → Validation Steps: {validation_steps}\")\n",
    "            logger.info(f\"  → Training Samples: {self.train_generator.samples}\")\n",
    "            logger.info(f\"  → Validation Samples: {self.validation_generator.samples}\")\n",
    "            \n",
    "            # ============================================================\n",
    "            # MLFLOW: Start Run with DagsHub\n",
    "            # ============================================================\n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"MLFLOW: Starting Experiment Tracking with DagsHub\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            # Initialize DagsHub\n",
    "            dagshub.init(\n",
    "                repo_owner='rahul22106',\n",
    "                repo_name='Brain-Tumor-Classification',\n",
    "                mlflow=True\n",
    "            )\n",
    "            logger.info(\"✓ DagsHub initialized successfully\")\n",
    "            \n",
    "            # Set MLflow tracking URI (if provided)\n",
    "            if self.mlflow_config.tracking_uri:\n",
    "                mlflow.set_tracking_uri(self.mlflow_config.tracking_uri)\n",
    "                logger.info(f\"✓ MLflow Tracking URI: {self.mlflow_config.tracking_uri}\")\n",
    "            else:\n",
    "                logger.info(\"✓ MLflow Tracking: Local (mlruns folder)\")\n",
    "            \n",
    "            # Set experiment name\n",
    "            mlflow.set_experiment(self.mlflow_config.experiment_name)\n",
    "            \n",
    "            # Generate run name\n",
    "            run_name = f\"{self.mlflow_config.run_name_prefix}_batch{self.config.params_batch_size}_epoch{self.config.params_epochs}\"\n",
    "            \n",
    "            with mlflow.start_run(run_name=run_name) as run:\n",
    "                run_id = run.info.run_id\n",
    "                logger.info(f\"✓ MLflow Run ID: {run_id}\")\n",
    "                logger.info(f\"✓ Run Name: {run_name}\")\n",
    "                logger.info(f\"✓ Experiment: {self.mlflow_config.experiment_name}\")\n",
    "                logger.info(f\"✓ View on DagsHub: https://dagshub.com/rahul22106/Brain-Tumor-Classification/experiments\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # MLFLOW: Log Parameters\n",
    "                # ============================================================\n",
    "                logger.info(\"\\nLogging Parameters to MLflow...\")\n",
    "\n",
    "                mlflow.log_param(\"model_architecture\", \"MobileNetV2\")\n",
    "                mlflow.log_param(\"image_size\", f\"{self.config.params_image_size[0]}x{self.config.params_image_size[1]}\")\n",
    "                mlflow.log_param(\"batch_size\", self.config.params_batch_size)\n",
    "                mlflow.log_param(\"epochs\", self.config.params_epochs)\n",
    "\n",
    "                # ✅ FIX: Use config learning rate instead of model.optimizer\n",
    "                mlflow.log_param(\"learning_rate\", self.config.params_learning_rate)\n",
    "\n",
    "                mlflow.log_param(\"augmentation\", str(self.config.params_is_augmentation))\n",
    "                mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "                mlflow.log_param(\"loss_function\", \"categorical_crossentropy\")\n",
    "                mlflow.log_param(\"train_samples\", int(self.train_generator.samples))\n",
    "                mlflow.log_param(\"val_samples\", int(self.validation_generator.samples))\n",
    "                mlflow.log_param(\"num_classes\", int(self.train_generator.num_classes))\n",
    "                # Convert list to comma-separated string\n",
    "                mlflow.log_param(\"class_names\", \",\".join(self.train_generator.class_indices.keys()))\n",
    "\n",
    "                logger.info(\"✓ Parameters logged to MLflow\")\n",
    "                                \n",
    "                # ============================================================\n",
    "                # MLFLOW: Set Tags\n",
    "                # ============================================================\n",
    "                # Set tags from config\n",
    "                for tag_key, tag_value in self.mlflow_config.tags.items():\n",
    "                                    mlflow.set_tag(tag_key, tag_value)\n",
    "                                \n",
    "                logger.info(\"✓ Tags set in MLflow\")\n",
    "                                \n",
    "                # ============================================================\n",
    "                # Callbacks Setup\n",
    "                # ============================================================\n",
    "                logger.info(\"\\nSetting up enhanced callbacks...\")\n",
    "\n",
    "                # Model checkpoint - save best model\n",
    "                checkpoint_path = str(self.config.trained_model_path).replace('.keras', '_best.keras')\n",
    "                checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=checkpoint_path,\n",
    "                    monitor='val_accuracy',  # Track accuracy\n",
    "                    save_best_only=True,\n",
    "                    mode='max',\n",
    "                    verbose=1\n",
    "                )\n",
    "                logger.info(f\"✓ ModelCheckpoint: {checkpoint_path}\")\n",
    "\n",
    "                # Early stopping with more patience\n",
    "                early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
    "                    patience=8,            # Increased patience\n",
    "                    restore_best_weights=True,\n",
    "                    mode='max',\n",
    "                    verbose=1\n",
    "                )\n",
    "                logger.info(\"✓ EarlyStopping: patience=12 (monitoring val_accuracy)\")\n",
    "\n",
    "                # Reduce learning rate on plateau with better settings\n",
    "                reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_accuracy',  # Monitor accuracy\n",
    "                    factor=0.5,             # Less aggressive reduction\n",
    "                    patience=5,             # More patience\n",
    "                    min_lr=1e-7,\n",
    "                    mode='max',\n",
    "                    verbose=1\n",
    "                )\n",
    "                logger.info(\"✓ ReduceLROnPlateau: factor=0.5, patience=5\")\n",
    "\n",
    "                # CSV Logger\n",
    "                csv_logger = tf.keras.callbacks.CSVLogger(\n",
    "                    filename=str(self.config.root_dir / 'training_log.csv'),\n",
    "                    append=True\n",
    "                )\n",
    "                logger.info(f\"✓ CSVLogger: {self.config.root_dir / 'training_log.csv'}\")\n",
    "\n",
    "                # TensorBoard\n",
    "                tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "                    log_dir=str(self.config.root_dir / 'tensorboard_logs'),\n",
    "                    histogram_freq=1,\n",
    "                    update_freq='epoch'\n",
    "                )\n",
    "                logger.info(f\"✓ TensorBoard: {self.config.root_dir / 'tensorboard_logs'}\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # Start Training\n",
    "                # ============================================================\n",
    "                logger.info(\"\\n\" + \"=\"*70)\n",
    "                logger.info(\"TRAINING STARTED\")\n",
    "                logger.info(\"=\"*70)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                history = self.model.fit(\n",
    "                    self.train_generator,\n",
    "                    epochs=self.config.params_epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[checkpoint, early_stop, reduce_lr, csv_logger, tensorboard],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                training_time = end_time - start_time\n",
    "                \n",
    "                logger.info(\"\\n\" + \"=\"*70)\n",
    "                logger.info(\"TRAINING COMPLETED\")\n",
    "                logger.info(\"=\"*70)\n",
    "                logger.info(f\"✓ Total training time: {training_time/60:.2f} minutes\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # MLFLOW: Log Final Metrics\n",
    "                # ============================================================\n",
    "                logger.info(\"\\n\" + \"=\"*70)\n",
    "                logger.info(\"LOGGING FINAL METRICS TO MLFLOW\")\n",
    "                logger.info(\"=\"*70)\n",
    "                \n",
    "                final_train_acc = history.history['accuracy'][-1]\n",
    "                final_val_acc = history.history['val_accuracy'][-1]\n",
    "                final_train_loss = history.history['loss'][-1]\n",
    "                final_val_loss = history.history['val_loss'][-1]\n",
    "                \n",
    "                best_val_acc = max(history.history['val_accuracy'])\n",
    "                best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "                \n",
    "                # Log final metrics - use simple metric logging without steps\n",
    "                mlflow.log_metric(\"final_train_accuracy\", final_train_acc)\n",
    "                mlflow.log_metric(\"final_val_accuracy\", final_val_acc)\n",
    "                mlflow.log_metric(\"final_train_loss\", final_train_loss)\n",
    "                mlflow.log_metric(\"final_val_loss\", final_val_loss)\n",
    "                mlflow.log_metric(\"best_val_accuracy\", best_val_acc)\n",
    "                mlflow.log_metric(\"training_time_minutes\", training_time/60)\n",
    "                \n",
    "                logger.info(\"✓ Final metrics logged to MLflow\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # Training Summary\n",
    "                # ============================================================\n",
    "                logger.info(\"\\n\" + \"=\"*70)\n",
    "                logger.info(\"TRAINING SUMMARY\")\n",
    "                logger.info(\"=\"*70)\n",
    "                \n",
    "                logger.info(f\"Final Training Accuracy: {final_train_acc*100:.2f}%\")\n",
    "                logger.info(f\"Final Validation Accuracy: {final_val_acc*100:.2f}%\")\n",
    "                logger.info(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "                logger.info(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "                logger.info(f\"\\nBest Validation Accuracy: {best_val_acc*100:.2f}% (Epoch {best_epoch})\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # Save Model\n",
    "                # ============================================================\n",
    "                logger.info(\"\\n\" + \"=\"*70)\n",
    "                logger.info(\"SAVING MODEL\")\n",
    "                logger.info(\"=\"*70)\n",
    "                \n",
    "                self.save_model(\n",
    "                    path=self.config.trained_model_path,\n",
    "                    model=self.model\n",
    "                )\n",
    "                logger.info(f\"✓ Final model saved\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # MLFLOW: Log Model Artifacts (Optional - DagsHub might not support all operations)\n",
    "                # ============================================================\n",
    "                logger.info(\"\\n\" + \"=\"*70)\n",
    "                logger.info(\"LOGGING ARTIFACTS TO MLFLOW\")\n",
    "                logger.info(\"=\"*70)\n",
    "                \n",
    "                try:\n",
    "                    # Log the model (simplified approach)\n",
    "                    mlflow.tensorflow.log_model(\n",
    "                        self.model,\n",
    "                        artifact_path=\"model\"\n",
    "                    )\n",
    "                    logger.info(\"✓ Model logged to MLflow\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not log model to MLflow: {e}\")\n",
    "                    logger.info(\"Continuing without model logging...\")\n",
    "                \n",
    "                try:\n",
    "                    # Log training history CSV\n",
    "                    mlflow.log_artifact(str(self.config.root_dir / 'training_log.csv'))\n",
    "                    logger.info(\"✓ Training log CSV logged to MLflow\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not log training log: {e}\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # MLFLOW: End Run\n",
    "                # ============================================================\n",
    "                logger.info(\"\\n\" + \"=\"*70)\n",
    "                logger.info(\"MLFLOW: Experiment Tracking Completed\")\n",
    "                logger.info(\"=\"*70)\n",
    "                logger.info(f\"✓ Run ID: {run_id}\")\n",
    "                logger.info(f\"✓ View on DagsHub: https://dagshub.com/rahul22106/Brain-Tumor-Classification/experiments\")\n",
    "                logger.info(f\"✓ Or MLflow UI: mlflow ui → http://localhost:5000\")\n",
    "                \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    \n",
    "    def initiate_model_training(self):\n",
    "        \"\"\"\n",
    "        Main method to execute model training with MLflow\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(\"INITIALIZING MODEL TRAINING WITH MLFLOW\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            # Step 1: Load base model\n",
    "            logger.info(\"\\n>>> Step 1: Load Base Model\")\n",
    "            self.get_base_model()\n",
    "            \n",
    "            # Step 2: Setup data generators\n",
    "            logger.info(\"\\n>>> Step 2: Setup Data Generators\")\n",
    "            self.train_valid_generator()\n",
    "            \n",
    "            # Step 3: Train model with MLflow tracking\n",
    "            logger.info(\"\\n>>> Step 3: Train Model with MLflow Tracking\")\n",
    "            history = self.train()\n",
    "            \n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"✓ MODEL TRAINING COMPLETED SUCCESSFULLY\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            logger.info(\"\\nNext Steps:\")\n",
    "            logger.info(\"  1. View MLflow UI: mlflow ui\")\n",
    "            logger.info(\"  2. Check training logs: artifacts/training/training_log.csv\")\n",
    "            logger.info(\"  3. View TensorBoard: tensorboard --logdir artifacts/training/tensorboard_logs\")\n",
    "            logger.info(\"  4. Evaluate model on test set (Stage 04)\")\n",
    "            logger.info(\"  5. Best model: \" + str(self.config.trained_model_path).replace('.keras', '_best.keras'))\n",
    "            \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 03: Model Training\n",
    "STAGE_NAME = \"Model Training Stage\"\n",
    "try:\n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
    "    logger.info(f\"{'='*70}\\n\")\n",
    "    \n",
    "    model_training = ModelTrainingPipeline()\n",
    "    model_training.main()\n",
    "    \n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\")\n",
    "    logger.info(f\"{'='*70}\\n\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

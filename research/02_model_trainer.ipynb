{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9faeb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ebf35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\shipr\\\\OneDrive\\\\Documents\\\\project\\\\Brain-tumor-Classification\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd2ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e8efc5",
   "metadata": {},
   "source": [
    "### Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bbd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for Model Training\"\"\"\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MLflowConfig:\n",
    "    \"\"\"Configuration for MLflow Tracking\"\"\"\n",
    "    experiment_name: str\n",
    "    run_name_prefix: str\n",
    "    tracking_uri: str\n",
    "    registered_model_name: str\n",
    "    tags: dict     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252013d3",
   "metadata": {},
   "source": [
    "### Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BT_Classification.constants import *\n",
    "from BT_Classification.utils.common import read_yaml, create_directories\n",
    "from BT_Classification.entity import (\n",
    "    DataIngestionConfig,PrepareBaseModelConfig,MLflowConfig,TrainingConfig\n",
    ")\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Configuration Manager to read and manage all configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Create artifacts root directory\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Get Data Ingestion configuration\n",
    "        \n",
    "        Returns:\n",
    "            DataIngestionConfig: Configuration object for data ingestion\n",
    "        \"\"\"\n",
    "        config = self.config.data_ingestion\n",
    "        \n",
    "        # Create root directory for data ingestion\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=Path(config.local_data_file),\n",
    "            unzip_dir=Path(config.unzip_dir),\n",
    "            train_data_dir=Path(config.train_data_dir),\n",
    "            test_data_dir=Path(config.test_data_dir)\n",
    "        )\n",
    "        \n",
    "        return data_ingestion_config\n",
    "    \n",
    "     \n",
    "    def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "        \"\"\"\n",
    "        Get Base Model Preparation configuration\n",
    "        \n",
    "        Returns:\n",
    "            PrepareBaseModelConfig: Configuration object for base model\n",
    "        \"\"\"\n",
    "        config = self.config.prepare_base_model\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        prepare_base_model_config = PrepareBaseModelConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            base_model_path=Path(config.base_model_path),\n",
    "            updated_base_model_path=Path(config.updated_base_model_path),\n",
    "            params_image_size=self.params.IMAGE_SIZE,\n",
    "            params_learning_rate=self.params.LEARNING_RATE,\n",
    "            params_include_top=self.params.INCLUDE_TOP,\n",
    "            params_weights=self.params.WEIGHTS,\n",
    "            params_classes=self.params.CLASSES\n",
    "        )\n",
    "        \n",
    "        return prepare_base_model_config\n",
    "    \n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        \"\"\"\n",
    "        Get Training configuration\n",
    "        \n",
    "        Returns:\n",
    "            TrainingConfig: Configuration object for model training\n",
    "        \"\"\"\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        training_data = self.config.data_ingestion.train_data_dir\n",
    "        \n",
    "        create_directories([Path(training.root_dir)])\n",
    "        \n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_is_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            params_learning_rate=params.LEARNING_RATE\n",
    "        )\n",
    "        \n",
    "        return training_config\n",
    "    \n",
    "    \n",
    "    def get_mlflow_config(self) -> MLflowConfig:\n",
    "        \"\"\"\n",
    "        Get MLflow configuration\n",
    "        \n",
    "        Returns:\n",
    "            MLflowConfig: Configuration object for MLflow tracking\n",
    "        \"\"\"\n",
    "        mlflow_config_data = self.config.mlflow\n",
    "        \n",
    "        mlflow_config = MLflowConfig(\n",
    "            experiment_name=mlflow_config_data.experiment_name,\n",
    "            run_name_prefix=mlflow_config_data.run_name_prefix,\n",
    "            tracking_uri=mlflow_config_data.tracking_uri if mlflow_config_data.tracking_uri else \"\",\n",
    "            registered_model_name=mlflow_config_data.registered_model_name,\n",
    "            tags=dict(mlflow_config_data.tags) if hasattr(mlflow_config_data, 'tags') else {}\n",
    "        )\n",
    "        \n",
    "        return mlflow_config\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76a60b",
   "metadata": {},
   "source": [
    "### 02_model_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3301e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import dagshub\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from BT_Classification import logger\n",
    "from BT_Classification.entity import TrainingConfig, MLflowConfig\n",
    "\n",
    "\n",
    "class Training:\n",
    "    \"\"\"\n",
    "    FINAL OPTIMIZED Training - Balanced approach based on best results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, mlflow_config: MLflowConfig):\n",
    "        self.config = config\n",
    "        self.mlflow_config = mlflow_config\n",
    "    \n",
    "    def get_base_model(self):\n",
    "        \"\"\"Load the prepared base model\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading improved model from: {self.config.updated_base_model_path}\")\n",
    "            \n",
    "            self.model = tf.keras.models.load_model(\n",
    "                str(self.config.updated_base_model_path)\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Improved model loaded successfully\")\n",
    "            logger.info(f\"Model input shape: {self.model.input_shape}\")\n",
    "            logger.info(f\"Model output shape: {self.model.output_shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    def train_valid_generator(self):\n",
    "        \"\"\"Create OPTIMIZED data generators\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Setting up OPTIMIZED data generators...\")\n",
    "            \n",
    "            if self.config.params_is_augmentation:\n",
    "                logger.info(\"Optimized augmentation enabled - BALANCED approach\")\n",
    "                train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    rotation_range=30,\n",
    "                    width_shift_range=0.3,\n",
    "                    height_shift_range=0.3,\n",
    "                    shear_range=0.3,\n",
    "                    zoom_range=0.35,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    brightness_range=[0.7, 1.3],\n",
    "                    fill_mode='nearest',\n",
    "                    validation_split=0.2\n",
    "                )\n",
    "            else:\n",
    "                train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    validation_split=0.2\n",
    "                )\n",
    "            \n",
    "            # Training generator\n",
    "            self.train_generator = train_datagen.flow_from_directory(\n",
    "                directory=str(self.config.training_data),\n",
    "                target_size=self.config.params_image_size[:-1],\n",
    "                batch_size=self.config.params_batch_size,\n",
    "                class_mode='categorical',\n",
    "                subset='training',\n",
    "                shuffle=True,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            # Validation generator (no augmentation)\n",
    "            self.validation_generator = train_datagen.flow_from_directory(\n",
    "                directory=str(self.config.training_data),\n",
    "                target_size=self.config.params_image_size[:-1],\n",
    "                batch_size=self.config.params_batch_size,\n",
    "                class_mode='categorical',\n",
    "                subset='validation',\n",
    "                shuffle=False,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(\"DATASET INFORMATION\")\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(f\"Training samples: {self.train_generator.samples}\")\n",
    "            logger.info(f\"Validation samples: {self.validation_generator.samples}\")\n",
    "            logger.info(f\"Number of classes: {self.train_generator.num_classes}\")\n",
    "            logger.info(f\"Class indices: {self.train_generator.class_indices}\")\n",
    "            logger.info(f\"Batch size: {self.config.params_batch_size}\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        try:\n",
    "            model.save(str(path))\n",
    "            logger.info(f\"Model saved at: {path}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train with FINAL OPTIMIZED class weights\"\"\"\n",
    "        try:\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(\"STARTING FINAL OPTIMIZED TRAINING\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size\n",
    "            validation_steps = self.validation_generator.samples // self.validation_generator.batch_size\n",
    "            \n",
    "            logger.info(f\"\\nTraining Configuration:\")\n",
    "            logger.info(f\"  Epochs: {self.config.params_epochs}\")\n",
    "            logger.info(f\"  Batch Size: {self.config.params_batch_size}\")\n",
    "            logger.info(f\"  Steps per Epoch: {steps_per_epoch}\")\n",
    "            logger.info(f\"  Validation Steps: {validation_steps}\")\n",
    "            \n",
    "        # ==================== FINAL OPTIMIZED CLASS WEIGHTS ====================\n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"CALCULATING FINAL OPTIMIZED CLASS WEIGHTS\")\n",
    "            logger.info(\"=\"*70)\n",
    "\n",
    "            # Get base balanced weights\n",
    "            class_weights_array = class_weight.compute_class_weight(\n",
    "                class_weight='balanced',\n",
    "                classes=np.unique(self.train_generator.classes),\n",
    "                y=self.train_generator.classes\n",
    "            )\n",
    "\n",
    "            class_weight_dict = dict(enumerate(class_weights_array))\n",
    "\n",
    "            # FINAL OPTIMIZED: Based on Run 1 results (which worked best)\n",
    "            # Analysis: 3x was too weak for Glioma, 6x hurt other classes\n",
    "            # Optimal: 4.2x Glioma, reduce No Tumor more aggressively\n",
    "            performance_boost_factors = {\n",
    "                0: 4.2,  # Glioma: Optimal boost (between 3x and 6x)\n",
    "                1: 1.0,  # Meningioma: Keep balanced (was perfect at 96%)\n",
    "                2: 0.45, # No Tumor: Reduce more (was still over-predicting)\n",
    "                3: 1.6   # Pituitary: Slight increase (86% → target 90%)\n",
    "            }\n",
    "\n",
    "            # Apply performance adjustments\n",
    "            logger.info(\"Base balanced weights + FINAL OPTIMIZED adjustments:\")\n",
    "            for class_idx, weight in class_weight_dict.items():\n",
    "                class_name = list(self.train_generator.class_indices.keys())[\n",
    "                    list(self.train_generator.class_indices.values()).index(class_idx)\n",
    "                ]\n",
    "                \n",
    "                original_weight = weight\n",
    "                boost_factor = performance_boost_factors.get(class_idx, 1.0)\n",
    "                adjusted_weight = original_weight * boost_factor\n",
    "                class_weight_dict[class_idx] = adjusted_weight\n",
    "                \n",
    "                logger.info(f\"  {class_name} (class {class_idx}): {original_weight:.4f} → {adjusted_weight:.4f} (x{boost_factor})\")\n",
    "\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(\"STRATEGY: Balanced optimization - 4.2x Glioma, control No Tumor\")\n",
    "        # =================================================================\n",
    "            \n",
    "            # Initialize MLflow\n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"MLFLOW: Starting Experiment\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            dagshub.init(\n",
    "                repo_owner='rahul22106',\n",
    "                repo_name='Brain-Tumor-Classification',\n",
    "                mlflow=True\n",
    "            )\n",
    "            \n",
    "            if self.mlflow_config.tracking_uri:\n",
    "                mlflow.set_tracking_uri(self.mlflow_config.tracking_uri)\n",
    "            \n",
    "            mlflow.set_experiment(self.mlflow_config.experiment_name)\n",
    "            \n",
    "            run_name = f\"{self.mlflow_config.run_name_prefix}_final_optimized_v4_batch{self.config.params_batch_size}\"\n",
    "            \n",
    "            with mlflow.start_run(run_name=run_name) as run:\n",
    "                run_id = run.info.run_id\n",
    "                logger.info(f\"MLflow Run ID: {run_id}\")\n",
    "                logger.info(f\"Strategy: FINAL OPTIMIZED - BALANCED PERFORMANCE\")\n",
    "                \n",
    "                # Log Parameters\n",
    "                mlflow.log_param(\"strategy\", \"final_optimized_balanced_v4\")\n",
    "                mlflow.log_param(\"model_architecture\", \"MobileNetV2_Improved\")\n",
    "                mlflow.log_param(\"batch_size\", self.config.params_batch_size)\n",
    "                mlflow.log_param(\"epochs\", self.config.params_epochs)\n",
    "                mlflow.log_param(\"learning_rate\", self.config.params_learning_rate)\n",
    "                mlflow.log_param(\"dropout\", \"0.35_0.35_0.3\")\n",
    "                mlflow.log_param(\"l2_reg\", \"0.008_0.008_0.005\")\n",
    "                mlflow.log_param(\"frozen_layers\", 100)\n",
    "                mlflow.log_param(\"gradient_clipping\", 1.0)\n",
    "                mlflow.log_param(\"class_weights\", str(class_weight_dict))\n",
    "                mlflow.log_param(\"augmentation\", \"optimized_balanced\")\n",
    "                mlflow.log_param(\"glioma_boost\", \"4.2x_optimal\")\n",
    "                \n",
    "                # Set Tags\n",
    "                for tag_key, tag_value in self.mlflow_config.tags.items():\n",
    "                    mlflow.set_tag(tag_key, tag_value)\n",
    "                mlflow.set_tag(\"training_strategy\", \"final_optimized_v4\")\n",
    "                mlflow.set_tag(\"focus\", \"balanced_all_classes\")\n",
    "                \n",
    "                # Callbacks\n",
    "                logger.info(\"\\nSetting up OPTIMIZED callbacks...\")\n",
    "\n",
    "                checkpoint_path = str(self.config.trained_model_path).replace('.keras', '_best.keras')\n",
    "                checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=checkpoint_path,\n",
    "                    monitor='val_accuracy',\n",
    "                    save_best_only=True,\n",
    "                    mode='max',\n",
    "                    verbose=1\n",
    "                )\n",
    "\n",
    "                # OPTIMIZED: Early stopping to prevent overfitting\n",
    "                early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_accuracy',  \n",
    "                    patience=10,\n",
    "                    restore_best_weights=True,\n",
    "                    mode='max',\n",
    "                    verbose=1,\n",
    "                    min_delta=0.001\n",
    "                )\n",
    "                logger.info(\"EarlyStopping: monitor=val_accuracy, patience=10, min_delta=0.001\")\n",
    "\n",
    "                reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.3,\n",
    "                    patience=3,\n",
    "                    min_lr=1e-7,\n",
    "                    mode='min',\n",
    "                    verbose=1\n",
    "                )\n",
    "                logger.info(\"ReduceLROnPlateau: factor=0.3, patience=3\")\n",
    "\n",
    "                csv_logger = tf.keras.callbacks.CSVLogger(\n",
    "                    filename=str(self.config.root_dir / 'training_log_final_v4.csv'),\n",
    "                    append=True\n",
    "                )\n",
    "\n",
    "                tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "                    log_dir=str(self.config.root_dir / 'tensorboard_logs_final_v4'),\n",
    "                    histogram_freq=1\n",
    "                )\n",
    "                \n",
    "                # Balanced performance monitor\n",
    "                class BalancedMonitor(tf.keras.callbacks.Callback):\n",
    "                    def on_epoch_end(self, epoch, logs=None):\n",
    "                        train_acc = logs.get('accuracy', 0)\n",
    "                        val_acc = logs.get('val_accuracy', 0)\n",
    "                        gap = train_acc - val_acc\n",
    "                        \n",
    "                        if gap > 0.15:\n",
    "                            logger.warning(f\"⚠️ OVERFITTING: Train-Val gap = {gap*100:.1f}%\")\n",
    "                        elif gap < -0.05:\n",
    "                            logger.warning(f\"⚠️ UNDERFITTING: Val > Train by {abs(gap)*100:.1f}%\")\n",
    "                        else:\n",
    "                            logger.info(f\"✓ Good balance\")\n",
    "                        \n",
    "                        logger.info(f\"Epoch {epoch+1}: Train={train_acc*100:.2f}% | Val={val_acc*100:.2f}% | Gap={gap*100:.1f}%\")\n",
    "                        logger.info(f\"  Strategy: 4.2x Glioma (optimal), 0.45x No Tumor (controlled)\")\n",
    "                \n",
    "                balanced_monitor = BalancedMonitor()\n",
    "                \n",
    "                # Start Training\n",
    "                logger.info(\"\\n\" + \"=\"*70)\n",
    "                logger.info(\"TRAINING STARTED - FINAL OPTIMIZED MODE\")\n",
    "                logger.info(\"=\"*70)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                history = self.model.fit(\n",
    "                    self.train_generator,\n",
    "                    epochs=self.config.params_epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    validation_steps=validation_steps,\n",
    "                    class_weight=class_weight_dict,\n",
    "                    callbacks=[checkpoint, early_stop, reduce_lr, csv_logger, tensorboard, balanced_monitor],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                training_time = end_time - start_time\n",
    "                \n",
    "                logger.info(\"\\n\" + \"=\"*70)\n",
    "                logger.info(\"TRAINING COMPLETED\")\n",
    "                logger.info(\"=\"*70)\n",
    "                logger.info(f\"Total training time: {training_time/60:.2f} minutes\")\n",
    "                \n",
    "                # Log Final Metrics\n",
    "                final_train_acc = history.history['accuracy'][-1]\n",
    "                final_val_acc = history.history['val_accuracy'][-1]\n",
    "                final_train_loss = history.history['loss'][-1]\n",
    "                final_val_loss = history.history['val_loss'][-1]\n",
    "                \n",
    "                best_val_acc = max(history.history['val_accuracy'])\n",
    "                best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "                \n",
    "                train_val_gap = final_train_acc - final_val_acc\n",
    "                \n",
    "                mlflow.log_metric(\"final_train_accuracy\", final_train_acc)\n",
    "                mlflow.log_metric(\"final_val_accuracy\", final_val_acc)\n",
    "                mlflow.log_metric(\"final_train_loss\", final_train_loss)\n",
    "                mlflow.log_metric(\"final_val_loss\", final_val_loss)\n",
    "                mlflow.log_metric(\"best_val_accuracy\", best_val_acc)\n",
    "                mlflow.log_metric(\"train_val_gap\", train_val_gap)\n",
    "                mlflow.log_metric(\"training_time_minutes\", training_time/60)\n",
    "                \n",
    "                # Training Summary\n",
    "                logger.info(\"\\n\" + \"=\"*70)\n",
    "                logger.info(\"FINAL TRAINING SUMMARY\")\n",
    "                logger.info(\"=\"*70)\n",
    "                logger.info(f\"Final Training Accuracy: {final_train_acc*100:.2f}%\")\n",
    "                logger.info(f\"Final Validation Accuracy: {final_val_acc*100:.2f}%\")\n",
    "                logger.info(f\"Train-Val Gap: {train_val_gap*100:.1f}%\")\n",
    "                logger.info(f\"Best Validation Accuracy: {best_val_acc*100:.2f}% (Epoch {best_epoch})\")\n",
    "                logger.info(f\"Class Weights: Glioma=4.2x, Meningioma=1.0x, NoTumor=0.45x, Pituitary=1.6x\")\n",
    "                \n",
    "                if train_val_gap > 0.1:\n",
    "                    logger.warning(\"⚠️ OVERFITTING DETECTED - Consider more regularization\")\n",
    "                elif train_val_gap < -0.05:\n",
    "                    logger.warning(\"⚠️ UNDERFITTING - Model can learn more\")\n",
    "                else:\n",
    "                    logger.info(\"✓ Good generalization achieved\")\n",
    "                \n",
    "                # Save Model\n",
    "                self.save_model(\n",
    "                    path=self.config.trained_model_path,\n",
    "                    model=self.model\n",
    "                )\n",
    "                \n",
    "                logger.info(f\"\\n✓ Best model saved: {checkpoint_path}\")\n",
    "                logger.info(f\"✓ Final model saved: {self.config.trained_model_path}\")\n",
    "                \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    def initiate_model_training(self):\n",
    "        \"\"\"Main training execution\"\"\"\n",
    "        try:\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(\"FINAL OPTIMIZED BALANCED TRAINING PIPELINE\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            logger.info(\"\\n>>> Step 1: Load Improved Model\")\n",
    "            self.get_base_model()\n",
    "            \n",
    "            logger.info(\"\\n>>> Step 2: Setup Optimized Augmentation\")\n",
    "            self.train_valid_generator()\n",
    "            \n",
    "            logger.info(\"\\n>>> Step 3: Train with Optimal Weights (4.2x Glioma)\")\n",
    "            history = self.train()\n",
    "            \n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"TRAINING COMPLETED - FINAL OPTIMIZED VERSION\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110dcee3",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STAGE_NAME = \"Model Training Stage\"\n",
    "\n",
    "\n",
    "class ModelTrainingPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline for Model Training Stage\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def main(self):\n",
    "        \"\"\"Execute model training pipeline\"\"\"\n",
    "        config = ConfigurationManager()\n",
    "        training_config = config.get_training_config()\n",
    "        mlflow_config = config.get_mlflow_config()\n",
    "        training = Training(config=training_config, mlflow_config=mlflow_config)\n",
    "        training.initiate_model_training()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
    "        logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "        obj = ModelTrainingPipeline()\n",
    "        obj.main()\n",
    "        \n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\")\n",
    "        logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise e    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab4c42",
   "metadata": {},
   "source": [
    "### Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STAGE_NAME = \"Model Training Stage\"\n",
    "try:\n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
    "    logger.info(f\"{'='*70}\\n\")\n",
    "    \n",
    "    model_training = ModelTrainingPipeline()\n",
    "    model_training.main()\n",
    "    \n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\")\n",
    "    logger.info(f\"{'='*70}\\n\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 03: Model Training\n",
    "from BT_Classification.pipeline import ModelTrainingPipeline\n",
    "STAGE_NAME = \"Model Training Stage\"\n",
    "try:\n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
    "    logger.info(f\"{'='*70}\\n\")\n",
    "    \n",
    "    model_training = ModelTrainingPipeline()\n",
    "    model_training.main()\n",
    "    \n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\")\n",
    "    logger.info(f\"{'='*70}\\n\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

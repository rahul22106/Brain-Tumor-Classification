{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8710e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42011da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\shipr\\\\OneDrive\\\\Documents\\\\project\\\\Brain-tumor-Classification\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ada60b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8509c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d04f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    root_dir: Path\n",
    "    path_of_model: Path\n",
    "    test_data_dir: Path\n",
    "    metric_file_name: str\n",
    "    params_image_size: list\n",
    "    params_batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c087f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        \"\"\"Get evaluation configuration\"\"\"\n",
    "        config = self.config.evaluation\n",
    "        params = self.params\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        evaluation_config = EvaluationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            path_of_model=Path(config.path_of_model),\n",
    "            test_data_dir=Path(config.test_data_dir),\n",
    "            metric_file_name=config.metric_file_name,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            params_batch_size=params.BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        return evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e85918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import dagshub\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from BT_Classification import logger\n",
    "from BT_Classification.entity import EvaluationConfig, MLflowConfig\n",
    "from BT_Classification.utils.common import save_json\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    \"\"\"\n",
    "    Model Evaluation component with comprehensive metrics and MLflow logging\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: EvaluationConfig, mlflow_config: MLflowConfig):\n",
    "        self.config = config\n",
    "        self.mlflow_config = mlflow_config\n",
    "        self.model = None\n",
    "        self.test_generator = None\n",
    "        self.class_names = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained model\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading trained model from: {self.config.path_of_model}\")\n",
    "            \n",
    "            self.model = tf.keras.models.load_model(\n",
    "                str(self.config.path_of_model)\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Model loaded successfully\")\n",
    "            logger.info(f\"Model input shape: {self.model.input_shape}\")\n",
    "            logger.info(f\"Model output shape: {self.model.output_shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load and prepare test data\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading test data from: {self.config.test_data_dir}\")\n",
    "            \n",
    "            # Create test data generator (no augmentation)\n",
    "            test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                rescale=1./255\n",
    "            )\n",
    "            \n",
    "            self.test_generator = test_datagen.flow_from_directory(\n",
    "                directory=str(self.config.test_data_dir),\n",
    "                target_size=self.config.params_image_size[:-1],\n",
    "                batch_size=self.config.params_batch_size,\n",
    "                class_mode='categorical',\n",
    "                shuffle=False,  # Important for evaluation\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            self.class_names = list(self.test_generator.class_indices.keys())\n",
    "            \n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(\"TEST DATASET INFORMATION\")\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(f\"Test samples: {self.test_generator.samples}\")\n",
    "            logger.info(f\"Number of classes: {self.test_generator.num_classes}\")\n",
    "            logger.info(f\"Class names: {self.class_names}\")\n",
    "            logger.info(f\"Class indices: {self.test_generator.class_indices}\")\n",
    "            logger.info(f\"Batch size: {self.config.params_batch_size}\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate model and compute metrics\"\"\"\n",
    "        try:\n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"STARTING MODEL EVALUATION\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            # Calculate steps - FIXED: Convert to int\n",
    "            steps = int(np.ceil(self.test_generator.samples / self.test_generator.batch_size))\n",
    "            \n",
    "            logger.info(f\"Total test steps: {steps}\")\n",
    "            \n",
    "            # Get predictions\n",
    "            logger.info(\"Generating predictions...\")\n",
    "            predictions = self.model.predict(\n",
    "                self.test_generator,\n",
    "                steps=steps,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Get true labels - FIXED: Ensure correct length\n",
    "            true_labels = self.test_generator.classes[:len(predictions)]\n",
    "            predicted_labels = np.argmax(predictions, axis=1)\n",
    "            \n",
    "            logger.info(f\"Predictions shape: {predictions.shape}\")\n",
    "            logger.info(f\"True labels length: {len(true_labels)}\")\n",
    "            logger.info(f\"Predicted labels length: {len(predicted_labels)}\")\n",
    "            \n",
    "            # Calculate basic metrics\n",
    "            test_loss, test_accuracy = self.model.evaluate(\n",
    "                self.test_generator,\n",
    "                steps=steps,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"EVALUATION METRICS\")\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(f\"Test Loss: {test_loss:.4f}\")\n",
    "            logger.info(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "            \n",
    "            # Classification Report\n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"CLASSIFICATION REPORT\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            report = classification_report(\n",
    "                true_labels,\n",
    "                predicted_labels,\n",
    "                target_names=self.class_names,\n",
    "                output_dict=True,\n",
    "                zero_division=0\n",
    "            )\n",
    "            \n",
    "            # Print detailed report\n",
    "            print(classification_report(\n",
    "                true_labels,\n",
    "                predicted_labels,\n",
    "                target_names=self.class_names,\n",
    "                zero_division=0\n",
    "            ))\n",
    "            \n",
    "            # Confusion Matrix\n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"CONFUSION MATRIX\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            cm = confusion_matrix(true_labels, predicted_labels)\n",
    "            logger.info(f\"\\n{cm}\")\n",
    "            \n",
    "            # Calculate per-class accuracy\n",
    "            per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "            logger.info(\"\\nPer-Class Accuracy:\")\n",
    "            for i, class_name in enumerate(self.class_names):\n",
    "                logger.info(f\"  {class_name}: {per_class_accuracy[i]*100:.2f}%\")\n",
    "            \n",
    "            # Save confusion matrix plot\n",
    "            self.plot_confusion_matrix(cm, self.class_names)\n",
    "            \n",
    "            # Calculate AUC-ROC (if applicable)\n",
    "            try:\n",
    "                # Convert to one-hot encoding for AUC calculation\n",
    "                n_classes = len(self.class_names)\n",
    "                true_labels_onehot = np.eye(n_classes)[true_labels]\n",
    "                \n",
    "                # Make sure predictions match true_labels length\n",
    "                predictions_aligned = predictions[:len(true_labels)]\n",
    "                \n",
    "                # Calculate AUC for each class\n",
    "                auc_scores = {}\n",
    "                for i, class_name in enumerate(self.class_names):\n",
    "                    auc = roc_auc_score(\n",
    "                        true_labels_onehot[:, i],\n",
    "                        predictions_aligned[:, i]\n",
    "                    )\n",
    "                    auc_scores[class_name] = auc\n",
    "                    logger.info(f\"AUC-ROC for {class_name}: {auc:.4f}\")\n",
    "                \n",
    "                # Calculate macro-average AUC\n",
    "                macro_auc = np.mean(list(auc_scores.values()))\n",
    "                logger.info(f\"\\nMacro-Average AUC-ROC: {macro_auc:.4f}\")\n",
    "                \n",
    "                # Plot ROC curves\n",
    "                self.plot_roc_curves(true_labels_onehot, predictions_aligned, self.class_names)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not calculate AUC-ROC: {str(e)}\")\n",
    "                auc_scores = {}\n",
    "                macro_auc = None\n",
    "            \n",
    "            # Prepare metrics dictionary\n",
    "            metrics = {\n",
    "                \"test_loss\": float(test_loss),\n",
    "                \"test_accuracy\": float(test_accuracy),\n",
    "                \"macro_avg_precision\": float(report['macro avg']['precision']),\n",
    "                \"macro_avg_recall\": float(report['macro avg']['recall']),\n",
    "                \"macro_avg_f1_score\": float(report['macro avg']['f1-score']),\n",
    "                \"weighted_avg_precision\": float(report['weighted avg']['precision']),\n",
    "                \"weighted_avg_recall\": float(report['weighted avg']['recall']),\n",
    "                \"weighted_avg_f1_score\": float(report['weighted avg']['f1-score']),\n",
    "            }\n",
    "            \n",
    "            # Add per-class metrics\n",
    "            for class_name in self.class_names:\n",
    "                metrics[f\"{class_name}_precision\"] = float(report[class_name]['precision'])\n",
    "                metrics[f\"{class_name}_recall\"] = float(report[class_name]['recall'])\n",
    "                metrics[f\"{class_name}_f1_score\"] = float(report[class_name]['f1-score'])\n",
    "                metrics[f\"{class_name}_accuracy\"] = float(per_class_accuracy[self.class_names.index(class_name)])\n",
    "            \n",
    "            # Add AUC scores if available\n",
    "            if auc_scores:\n",
    "                for class_name, auc in auc_scores.items():\n",
    "                    metrics[f\"{class_name}_auc_roc\"] = float(auc)\n",
    "                if macro_auc:\n",
    "                    metrics[\"macro_avg_auc_roc\"] = float(macro_auc)\n",
    "            \n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"EVALUATION COMPLETED SUCCESSFULLY\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            return metrics, cm, report\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, class_names):\n",
    "        \"\"\"Plot and save confusion matrix\"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(\n",
    "                cm,\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'}\n",
    "            )\n",
    "            plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "            plt.ylabel('True Label', fontsize=12)\n",
    "            plt.xlabel('Predicted Label', fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            cm_path = Path(self.config.root_dir) / 'confusion_matrix.png'\n",
    "            plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            logger.info(f\"Confusion matrix saved at: {cm_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save confusion matrix plot: {str(e)}\")\n",
    "    \n",
    "    def plot_roc_curves(self, true_labels_onehot, predictions, class_names):\n",
    "        \"\"\"Plot and save ROC curves for all classes\"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            for i, class_name in enumerate(class_names):\n",
    "                fpr, tpr, _ = roc_curve(true_labels_onehot[:, i], predictions[:, i])\n",
    "                auc = roc_auc_score(true_labels_onehot[:, i], predictions[:, i])\n",
    "                plt.plot(fpr, tpr, label=f'{class_name} (AUC = {auc:.3f})', linewidth=2)\n",
    "            \n",
    "            plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate', fontsize=12)\n",
    "            plt.ylabel('True Positive Rate', fontsize=12)\n",
    "            plt.title('ROC Curves - Multi-Class Classification', fontsize=16, fontweight='bold')\n",
    "            plt.legend(loc=\"lower right\", fontsize=10)\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            roc_path = Path(self.config.root_dir) / 'roc_curves.png'\n",
    "            plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            logger.info(f\"ROC curves saved at: {roc_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save ROC curves plot: {str(e)}\")\n",
    "    \n",
    "    def save_evaluation_results(self, metrics):\n",
    "        \"\"\"Save evaluation metrics to JSON file\"\"\"\n",
    "        try:\n",
    "            scores_path = Path(self.config.root_dir) / self.config.metric_file_name\n",
    "            save_json(path=scores_path, data=metrics)\n",
    "            logger.info(f\"Evaluation metrics saved at: {scores_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    def log_to_mlflow(self, metrics, cm, report):\n",
    "        \"\"\"Log evaluation results to MLflow\"\"\"\n",
    "        try:\n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"LOGGING TO MLFLOW\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            # Initialize DagsHub (hardcoded from config.yaml)\n",
    "            dagshub.init(\n",
    "                repo_owner='rahul22106',\n",
    "                repo_name='Brain-Tumor-Classification',\n",
    "                mlflow=True\n",
    "            )\n",
    "            \n",
    "            if self.mlflow_config.tracking_uri:\n",
    "                mlflow.set_tracking_uri(self.mlflow_config.tracking_uri)\n",
    "            \n",
    "            mlflow.set_experiment(self.mlflow_config.experiment_name)\n",
    "            \n",
    "            run_name = f\"{self.mlflow_config.run_name_prefix}_evaluation\"\n",
    "            \n",
    "            with mlflow.start_run(run_name=run_name) as run:\n",
    "                run_id = run.info.run_id\n",
    "                logger.info(f\"MLflow Run ID: {run_id}\")\n",
    "                \n",
    "                # Log all metrics\n",
    "                logger.info(\"Logging metrics...\")\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    mlflow.log_metric(metric_name, metric_value)\n",
    "                \n",
    "                # Log model parameters\n",
    "                mlflow.log_param(\"model_path\", str(self.config.path_of_model))\n",
    "                mlflow.log_param(\"test_samples\", self.test_generator.samples)\n",
    "                mlflow.log_param(\"num_classes\", len(self.class_names))\n",
    "                mlflow.log_param(\"class_names\", str(self.class_names))\n",
    "                \n",
    "                # Set tags\n",
    "                for tag_key, tag_value in self.mlflow_config.tags.items():\n",
    "                    mlflow.set_tag(tag_key, tag_value)\n",
    "                mlflow.set_tag(\"stage\", \"evaluation\")\n",
    "                \n",
    "                # Log artifacts\n",
    "                logger.info(\"Logging artifacts...\")\n",
    "                \n",
    "                # Log confusion matrix\n",
    "                cm_path = Path(self.config.root_dir) / 'confusion_matrix.png'\n",
    "                if cm_path.exists():\n",
    "                    mlflow.log_artifact(str(cm_path))\n",
    "                \n",
    "                # Log ROC curves\n",
    "                roc_path = Path(self.config.root_dir) / 'roc_curves.png'\n",
    "                if roc_path.exists():\n",
    "                    mlflow.log_artifact(str(roc_path))\n",
    "                \n",
    "                # Log metrics JSON\n",
    "                scores_path = Path(self.config.root_dir) / self.config.metric_file_name\n",
    "                if scores_path.exists():\n",
    "                    mlflow.log_artifact(str(scores_path))\n",
    "                \n",
    "                # Log classification report as JSON\n",
    "                report_path = Path(self.config.root_dir) / 'classification_report.json'\n",
    "                with open(report_path, 'w') as f:\n",
    "                    json.dump(report, f, indent=4)\n",
    "                mlflow.log_artifact(str(report_path))\n",
    "                \n",
    "                logger.info(f\"✓ All artifacts logged to MLflow\")\n",
    "                logger.info(f\"✓ MLflow Run ID: {run_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    def evaluation(self):\n",
    "        \"\"\"Main evaluation pipeline\"\"\"\n",
    "        try:\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(\"MODEL EVALUATION PIPELINE\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            logger.info(\"\\n>>> Step 1: Load Trained Model\")\n",
    "            self.load_model()\n",
    "            \n",
    "            logger.info(\"\\n>>> Step 2: Load Test Data\")\n",
    "            self.load_test_data()\n",
    "            \n",
    "            logger.info(\"\\n>>> Step 3: Evaluate Model\")\n",
    "            metrics, cm, report = self.evaluate_model()\n",
    "            \n",
    "            logger.info(\"\\n>>> Step 4: Save Evaluation Results\")\n",
    "            self.save_evaluation_results(metrics)\n",
    "            \n",
    "            logger.info(\"\\n>>> Step 5: Log to MLflow\")\n",
    "            self.log_to_mlflow(metrics, cm, report)\n",
    "            \n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"EVALUATION PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            # Print final summary\n",
    "            logger.info(\"\\n\" + \"=\"*70)\n",
    "            logger.info(\"FINAL EVALUATION SUMMARY\")\n",
    "            logger.info(\"=\"*70)\n",
    "            logger.info(f\"Test Accuracy: {metrics['test_accuracy']*100:.2f}%\")\n",
    "            logger.info(f\"Test Loss: {metrics['test_loss']:.4f}\")\n",
    "            logger.info(f\"Macro F1-Score: {metrics['macro_avg_f1_score']:.4f}\")\n",
    "            logger.info(f\"Weighted F1-Score: {metrics['weighted_avg_f1_score']:.4f}\")\n",
    "            \n",
    "            if 'macro_avg_auc_roc' in metrics:\n",
    "                logger.info(f\"Macro AUC-ROC: {metrics['macro_avg_auc_roc']:.4f}\")\n",
    "            \n",
    "            logger.info(\"\\nPer-Class Performance:\")\n",
    "            for class_name in self.class_names:\n",
    "                acc = metrics[f\"{class_name}_accuracy\"]\n",
    "                f1 = metrics[f\"{class_name}_f1_score\"]\n",
    "                logger.info(f\"  {class_name}: Accuracy={acc*100:.2f}%, F1-Score={f1:.4f}\")\n",
    "            \n",
    "            logger.info(\"=\"*70)\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE_NAME = \"Model Evaluation Stage\"\n",
    "\n",
    "class ModelEvaluationPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline for Model Evaluation Stage\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def main(self):\n",
    "        \"\"\"Execute model evaluation pipeline\"\"\"\n",
    "        config = ConfigurationManager()\n",
    "        evaluation_config = config.get_evaluation_config()\n",
    "        mlflow_config = config.get_mlflow_config()\n",
    "        evaluation = Evaluation(config=evaluation_config, mlflow_config=mlflow_config)\n",
    "        evaluation.evaluation()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
    "        logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "        obj = ModelEvaluationPipeline()\n",
    "        obj.main()\n",
    "        \n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\")\n",
    "        logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise e   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
